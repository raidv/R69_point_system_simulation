# Competition Point System Optimizer

This project is a Python-based simulation framework designed to determine the most suitable point values ($X, Y, Z, C$) for a multi-stage competition. It runs numerous parameter combinations through Monte Carlo simulations to evaluate their impact on **competitive fairness, stability, and excitement**, using a single **Optimization Score** as the primary metric.

The simulation models a competition structured around six stages, with 12 participants drafting into three teams of four using a snake draft system based on current point totals. The core dynamic tests the balance between **initial skill** and the **weight of the point system** ($X, Y, Z, C$) combined with **randomness**.

---

## üõ†Ô∏è Project Structure and Setup

### Prerequisites

You need **Python 3.8+** installed. The project uses only standard Python libraries.

### Installation

No installation steps are necessary. Simply ensure your project directory contains all the required source files (`config.py`, `run_sweep.py`, etc.).

### 1. Configure Parameters

Edit the **`config.py`** file to define the range of integer point values you want to test and set the constants:

| Variable | Description | Example Value |
| :--- | :--- | :--- |
| `CHALLENGE_RAND` | Randomness factor for challenge outcomes ($0.0$ = pure skill, $1.0$ = pure random). | $0.3$ |
| `NUM_COMPETITIONS` | Number of times the full competition is simulated for **each** unique $X, Y, Z, C$ combination. Use $\ge 50$ for stable results. | $50$ |
| `X_LOWER/HIGHER`, etc. | Integer bounds for the point sweep. The sweep tests all combinations where $Z < X < Y$. | $X_{\text{LOWER}} = 5$, $X_{\text{HIGHER}} = 15$ |

### 2. Execute the Sweep

Run the main script from your terminal:

`python run_sweep.py`

---

## üìä Output and Analysis

The simulation generates all results into two structured directories, which are excluded from source control by the `.gitignore` file.

### 1. Optimization Sweep Leaderboard

This file is your primary source for finding the best point system, ranking all tested combinations by the **Optimization Score** (descending).

* **Files:** `optimization_sweep_leaderboard.txt` and `optimization_sweep_leaderboard.csv`.

| Metric (Column) | Goal | Meaning |
| :--- | :--- | :--- |
| **Optimization Score** | **Maximize** | The weighted aggregate metric. The **higher the score, the better the point system** meets all competitive goals. |
| **Stability** (Avg. Count) | $\ge 4.0$ | The average number of players who maintained their expected ranking band (e.g., Top 3 initial in Top 6 final). |
| **Collision** (Avg. Size) | $\le 3.0$ | The average size of the group tied at the critical 6th/7th place cut-off. **Lower is better.** |
| **Contenders** (Avg. Count) | $\ge 9.0$ | The average number of players still mathematically in contention for a Top 6 spot before the final stage. |

### 2. Detailed Logs

* **Location:** `sweep_results/`
* **Purpose:** Each file (e.g., `RESULTS_C1_Z2_X5_Y10.txt`) contains the **average leaderboard** and a full metric breakdown for one specific parameter set. This is where you conduct your detailed comparative analysis.

### Point Values and Their Roles

| Value | Challenge | Role in Competition |
| :--- | :--- | :--- |
| **$X$** | Challenge 1 (Team) | The initial team reward, providing momentum. |
| **$Y$** | Challenge 2 (Team) | The primary team reward ($Y > X$), driving major point shifts. |
| **$Z$** | Challenge 3 (Individual) | Team reward component for the winner of the individual challenge. |
| **$C$** | Challenge 3 (Individual) | The **volatility dial**. Individual bonus ($+C$) for the winner and penalty ($-C$) for the losers. |

***

$^*$ This README file, as well as the core logic and structure of the Python simulation code, was generated by Google's AI model, Gemini.